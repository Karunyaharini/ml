# -*- coding: utf-8 -*-
"""brain Tumor spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhhmuy9nRTrAMMUPESSKtAaAHjnucva1
"""

!pwd
# List files and folders
!ls
# Check the open jdk version on colab
!ls /usr/lib/jvm/

"""
Downloading and installing Java 8 JDK"""

# Download and install Java 8
!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!ls /usr/lib/jvm/

"""Installing Spark"""

# Download Apache Spark binary: This link can change based on the version. Update this link with the latest version before using
!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz

# Unzip file
!tar -xvzf spark-3.5.1-bin-hadoop3.tgz

# Install findspark
!pip install -q findspark

# Install pyspark
!pip install pyspark

# Add environmental variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"

# findspark will locate spark in the system
import findspark
findspark.init()

"""Creating Spark Session"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
        .master("local") \
        .appName("Brain Tumor Detection") \
        .getOrCreate()

print("spark session started!")

spark

spark.sparkContext

!pwd
!ls

spark_data = spark.read.format('csv').options(header='true',inferSchema='true').load("Brain Tumor.csv")
spark_data.show(5, truncate=False)

spark_data.printSchema()

spark_data.count()

pandas_df = spark_data.toPandas()
pandas_df.shape

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()          #Label Encoding
pandas_df['Image']= label_encoder.fit_transform(pandas_df['Image'])
print(pandas_df['Image'].head(10))

print(pandas_df.describe())

"""Splitting the Data"""

x1=pandas_df.drop('Class',axis=1)
y1=pandas_df['Class']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size=0.25, random_state=0)

"""Mutual Information Calculation"""

from sklearn.feature_selection import mutual_info_regression

from functools import reduce

map_list= {}
sorted_attribute_mi={}
def mapper(n):
      mutual_info_scores = mutual_info_regression(X_test, y_test)
      attribute_mi = {}
      for i, col in enumerate(X_test.columns):
          attribute_mi[col] = mutual_info_scores[i]
      sorted_attribute_mi = dict(sorted(attribute_mi.items(), key=lambda item: item[1], reverse=True))
      mi_scores_list = list(sorted_attribute_mi.values())
      for i in range(0,n-1):
        map_list[i]= mi_scores_list[i]
      return map_list

from collections import defaultdict
def reducer(map_list):
    val=map_list.values()
    my_val=list(val)
    max_mi=[]
    for i in my_val:
        if(i>0.1):
           max_mi.append(i)
    return max_mi

def map_reduce():
    n=15
    mapped_feature = mapper(n)
    reduced_feature= reducer(mapped_feature)
    return reduced_feature

red_rf = map_reduce()
print(red_rf)

def reduced_features(red_rf):
   red_features=[]
   mutual_info_scores = mutual_info_regression(X_test, y_test)
   attribute_mi={}
   for i, col in enumerate(X_test.columns):
          attribute_mi[col] = mutual_info_scores[i]
   for i in red_rf:
      for key, value in attribute_mi.items():
        if(i==value):
          red_features.append(key)

   return red_features

import numpy as np

def calculate_rrf_score(mutual_info_scores, alpha, TSamples):
    rrf_scores = {}
    for feature in mutual_info_scores.keys():
        rrf_score = mutual_info_scores[feature]
        for Aj in TSamples:
            if Aj != feature:
                rrf_score += np.power(mutual_info_scores[Aj], alpha)
        rrf_scores[feature] = rrf_score
    return rrf_scores

"""Reduced Resultant feature"""

res_red_f = reduced_features(red_rf)
print(res_red_f)

x2=pandas_df.drop(['Mean', 'Variance','Standard Deviation', 'Contrast','Correlation','Coarseness','Class','Image'], axis=1)
y2=pandas_df['Class']

from sklearn.model_selection import train_test_split

X_red_train, X_red_test, y_red_train, y_red_test = train_test_split(x2, y2, test_size=0.25, random_state=0)

print("Total data        :",pandas_df.shape[0])
print("No of test data   :",X_red_test.shape[0])
print("No of train data  :",X_red_train.shape[0])

from sklearn.decomposition import PCA

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.metrics import roc_curve, auc

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()                       #feature scaling
scaled_train = scaler.fit_transform(X_red_train)
scaled_test = scaler.fit_transform(X_red_test)

pca = PCA(n_components=6)
X_train_pca = pca.fit_transform(X_red_train)   # Applying PCA to reduce dimensionality
X_test_pca = pca.transform(X_red_test)

num_features_reduced = pandas_df.shape[1] - pca.n_components_
print(num_features_reduced )

import numpy as np
import pandas as pd

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, random_state=42)
X_embedded = tsne.fit_transform(X_red_train)

plt.figure(figsize=(6, 5))
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], alpha=0.7)
plt.title("t-SNE Visualization of Brain Tumor Dataset")
plt.xlabel("First t-SNE")
plt.ylabel("Second t-SNE")
plt.show()

!pip install deap

import time

import random
from deap import base, creator, tools, algorithms

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.metrics import roc_curve, auc

from sklearn.metrics import precision_score
beg_svm = time.time()
svm_classifier = SVC(kernel='rbf', random_state=42)

                                                       # Train SVM classifier
svm_classifier.fit(X_train_pca, y_red_train)

                                         # Prediction
y_pred = svm_classifier.predict(X_test_pca)
print("            Performance metrics       ")
accuracy1 = accuracy_score(y_red_test, y_pred)
print("Accuracy:", accuracy1*100,"%")

end_svm = time.time()

time_svm = end_svm - beg_svm
print("Classification Report:")
print(classification_report(y_red_test, y_pred))

print(" Prediction Time ",round(time_svm*1000))

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_red_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVM Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn.linear_model import LogisticRegression

beg_lr=time.time()
model = LogisticRegression()
model.fit(X_train_pca, y_red_train)

                           # Prediction on the test set
y_pred = model.predict(X_test_pca)

                             # Evaluating the model
accuracy2 = accuracy_score(y_red_test, y_pred)
print(f"Accuracy: {accuracy2*100:.2f}")
end_lr=time.time()
time_lr = end_lr - beg_lr


print("Classification Report:")
print(classification_report(y_red_test, y_pred))

print("Prediction Time",round(time_lr*1000))

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_red_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='purple', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn.naive_bayes import GaussianNB

beg_nb=time.time()
nb_classifier = GaussianNB()

               # Training Naive Bayes classifier
nb_classifier.fit(X_train_pca, y_red_train)

                   # Predictions
y_pred = nb_classifier.predict(X_test_pca)

                      # Evaluating the model
accuracy3 = accuracy_score(y_red_test, y_pred)
print("Accuracy:", accuracy3*100)
end_nb=time.time()
time_nb = end_nb - beg_nb

print("Classification Report:")
print(classification_report(y_red_test, y_pred))

print("Prediction Time",round(time_nb*1000))
# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_red_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='green', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Naive bayes Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn.ensemble import RandomForestClassifier

beg_rf=time.time()
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Training Random Forest classifier
rf_classifier.fit(X_train_pca, y_red_train)

# Prediction
y_pred = rf_classifier.predict(X_test_pca)

# Evaluating the model
accuracy4 = accuracy_score(y_red_test, y_pred)
print("Accuracy:", accuracy4*100)
end_rf=time.time()
time_rf=end_rf - beg_rf

print("Classification Report:")
print(classification_report(y_red_test, y_pred))

print("Prediction time",round(time_rf*1000))
# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_red_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='skyblue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

beg_st = time.time()
base_models = [
    RandomForestClassifier(),
    GradientBoostingClassifier()
]

base_model_predictions = np.zeros((len(X_test_pca), len(base_models)))

for i, model in enumerate(base_models):
    model.fit(X_train_pca, y_red_train)
    base_model_predictions[:, i] = model.predict(X_test_pca)

# Step 2: Train Meta-Model
meta_model = LogisticRegression()
meta_model.fit(base_model_predictions, y_red_test)

# Step 3: Make Predictions on Test Set
test_base_model_predictions = np.zeros((len(X_test_pca), len(base_models)))

for i, model in enumerate(base_models):
    test_base_model_predictions[:, i] = model.predict(X_test_pca)

final_predictions = meta_model.predict(test_base_model_predictions)

# Step 4: Evaluate
accuracy5 = accuracy_score(y_test, final_predictions)
print("Accuracy:", accuracy5*100)

end_st=time.time()
time_st=end_st - beg_st

print("prediction Time",round(time_st*1000))
fpr, tpr, thresholds = roc_curve(y_red_test, y_pred)

print("Classification Report:")
print(classification_report(y_red_test, final_predictions))

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='gold', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Stacking Curve')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

# List of algorithms
algorithms = ['SVM', 'Logistic Regression', 'Naive bayes', 'Random Forest', 'Stacking']

# Accuracy scores for each algorithm
accuracy_scores = [accuracy1, accuracy2 , accuracy3, accuracy4 , accuracy5]

# Creating the bar plot
plt.figure(figsize=(8, 4))
plt.bar(algorithms, accuracy_scores, color='pink')
plt.title('comparision on MI')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.ylim(0.2, 1.0)  # Setting y-axis limits
plt.show()

from random import random
from random import uniform

"""Particle Swarm Optimization"""

from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
preprocessing.LabelEncoder()
import random
import numpy as np
import warnings
warnings.filterwarnings("ignore")

def calaculate_accuracy(x):
    X=pandas_df.drop('Class',axis=1)
    y=pandas_df['Class']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state=30)
    lgt_R = LogisticRegression(random_state = 101, solver='lbfgs', multi_class='auto')
    lgt_R.fit(X_train, y_train)
    accuracy=lgt_R.score(X_test,y_test)
    print('Accuracy:',accuracy)
    return accuracy

columnsName=pandas_df.drop(labels= 'Class', axis= 1).columns.values.tolist()

columnsName1=[0,1]
tumorlist=[]
for i in range(10):
    tum1=[]
    for i in range(14):
        item = random.choice(tuple(columnsName1))
        tum1.append(item)
    tumorlist.append(tum1)

def data(tumorlist1):
    tumorlist2=[]
    for i in range(len(tumorlist1)-1):
        if tumorlist1[i]!=1:
                tumorlist2.append(columnsName[i])
    return tumorlist2

pb=[]
def checkpersonalnest():
    for i in range(len(tumorlist)):
         if tumorlist:  # Check if not empty
           pb.append(calaculate_accuracy(data(tumorlist[i])))

checkpersonalnest()

def checkvelocity(globalbest):
    velocity=[]
    for j in range(len(tumorlist)):
        velocity.append(list(0+1*(np.random.random(1)[0])*(np.array(tumorlist[j])-np.array(tumorlist[j]))+1*(np.random.random(1)[0])*(np.array(globalbest)-np.array(tumorlist[j]))))
    #print(velocity)
    return velocity

def addingtumor(velocity):
    tumorlist2=[]
    for i in range(len(velocity)):
        nexttum=[]
        for j in range(len(velocity[i])):
            nexttum.append(tumorlist[i][j]+velocity[i][j])
        tumorlist2.append(nexttum)
    return tumorlist2

def normalize(tumorlist2):
    for l in range(len(tumorlist2)):
        for m in range(len(tumorlist2[l])):
            if tumorlist2[l][m]>0.5:
                tumorlist2[l][m]=1
            else:
                tumorlist2[l][m]=0
    return tumorlist2

def checkpd(tumorlist2):
    personal=[]
    for i in range(len(tumorlist2)):
        personal.append(calaculate_accuracy(data(tumorlist2[i])))
    for j in range(len(personal)):
        if(personal[j]>pb[j]):
            tumorlist[j]=tumorlist2[j]
            pb[j]=personal[j]
    return personal

max(pb)
ind = pb.index(max(pb))
globalbest=tumorlist[ind]
for i in range(10):
    tumorlist2=[]
    personal=[]
    velocity=checkvelocity(globalbest)
    tumorlist2=addingtumor(velocity)
    tumorlist2=normalize(tumorlist2)
    personal=checkpd(tumorlist2)
    globalbest=[]
    max(pb)
    ind = pb.index(max(pb))
    globalbest=tumorlist[ind]

max(pb)

ind = pb.index(max(pb))
globalbest=tumorlist[ind]

print(data(globalbest))

X_pso=pandas_df.drop(['Variance','Class','Standard Deviation','Entropy','Skewness','ASM','Coarseness'],axis=1)
y_pso=pandas_df['Class']

from sklearn.model_selection import train_test_split

X_pso_train, X_pso_test, y_pso_train, y_pso_test = train_test_split(X_pso, y_pso, test_size=0.25, random_state=0)

from sklearn.decomposition import PCA


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()                       #feature scaling
scaled_train_pso = scaler.fit_transform(X_pso_train)
scaled_test_pso = scaler.fit_transform(X_pso_test)

pca = PCA(n_components=6)
X_train_pca1 = pca.fit_transform(scaled_train_pso)   # Applying PCA to reduce dimensionality
X_test_pca1 = pca.transform(scaled_test_pso)

num_features_reduced = pandas_df.shape[1] - pca.n_components_
print(num_features_reduced )

import numpy as np
import pandas as pd

tsne = TSNE(n_components=2, random_state=42)
X_embedded = tsne.fit_transform(X_train_pca1)

plt.figure(figsize=(6, 5))
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], alpha=0.7)
plt.title("t-SNE Visualization of Brain Tumor Dataset")
plt.xlabel("First t-SNE")
plt.ylabel("Second t-SNE")
plt.show()

!pip install deap

import time

import random
from deap import base, creator, tools, algorithms

beg_svm = time.time()
svm_classifier = SVC(kernel='rbf', random_state=42)

                                                       # Train SVM classifier
svm_classifier.fit(X_train_pca1, y_pso_train)

                                         # Prediction
y_pred = svm_classifier.predict(X_test_pca1)
print("            Performance metrics       ")
accuracy11 = accuracy_score(y_pso_test, y_pred)
print("Accuracy:", accuracy11*100,"%")

end_svm = time.time()

time_svm = end_svm - beg_svm
print("Classification Report:")
print(classification_report(y_pso_test, y_pred))

print(" Prediction Time ",round(time_svm*1000))

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_pso_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVM Curve')
plt.legend(loc='lower right')
plt.show()

beg_lr=time.time()
model = LogisticRegression()
model.fit(X_train_pca1, y_pso_train)

                           # Prediction on the test set
y_pred = model.predict(X_test_pca1)

                             # Evaluating the model
accuracy12 = accuracy_score(y_pso_test, y_pred)
print(f"Accuracy: {accuracy12*100:.2f}")
end_lr=time.time()
time_lr = end_lr - beg_lr


print("Classification Report:")
print(classification_report(y_pso_test, y_pred))

print("Prediction Time",round(time_lr*1000))

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_pso_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='purple', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression Curve')
plt.legend(loc='lower right')
plt.show()

beg_nb=time.time()
nb_classifier = GaussianNB()

               # Training Naive Bayes classifier
nb_classifier.fit(X_train_pca1, y_pso_train)

                   # Predictions
y_pred = nb_classifier.predict(X_test_pca1)

                      # Evaluating the model
accuracy13 = accuracy_score(y_pso_test, y_pred)
print("Accuracy:", accuracy13*100)
end_nb=time.time()
time_nb = end_nb - beg_nb

print("Classification Report:")
print(classification_report(y_pso_test, y_pred))

print("Prediction Time",round(time_nb*1000))
# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_pso_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='green', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Naive bayes Curve')
plt.legend(loc='lower right')
plt.show()

beg_rf=time.time()
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Training Random Forest classifier
rf_classifier.fit(X_train_pca1, y_pso_train)

# Prediction
y_pred = rf_classifier.predict(X_test_pca1)

# Evaluating the model
accuracy14 = accuracy_score(y_pso_test, y_pred)
print("Accuracy:", accuracy14*100)
end_rf=time.time()
time_rf=end_rf - beg_rf

print("Classification Report:")
print(classification_report(y_pso_test, y_pred))

print("Prediction time",round(time_rf*1000))
# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_pso_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='skyblue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest Curve')
plt.legend(loc='lower right')
plt.show()

beg_st = time.time()
base_models = [
    RandomForestClassifier(),
    GradientBoostingClassifier()
]

base_model_predictions = np.zeros((len(X_test_pca1), len(base_models)))

for i, model in enumerate(base_models):
    model.fit(X_train_pca1, y_pso_train)
    base_model_predictions[:, i] = model.predict(X_test_pca1)

# Step 2: Train Meta-Model
meta_model = LogisticRegression()
meta_model.fit(base_model_predictions, y_pso_test)

# Step 3: Make Predictions on Test Set
test_base_model_predictions = np.zeros((len(X_test_pca1), len(base_models)))

for i, model in enumerate(base_models):
    test_base_model_predictions[:, i] = model.predict(X_test_pca1)

final_predictions = meta_model.predict(test_base_model_predictions)

# Step 4: Evaluate
accuracy15 = accuracy_score(y_pso_test, final_predictions)
print("Accuracy:", accuracy15*100)

end_st=time.time()
time_st=end_st - beg_st

print("prediction Time",round(time_st*1000))
fpr, tpr, thresholds = roc_curve(y_pso_test, y_pred)

print("Classification Report:")
print(classification_report(y_pso_test, final_predictions))

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='gold', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Stacking Curve')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

# List of algorithms
algorithms = ['SVM', 'Logistic Regression', 'Naive bayes', 'Random Forest', 'Stacking']

# Accuracy scores for each algorithm
accuracy_scores = [accuracy11, accuracy12 , accuracy13, accuracy14 , accuracy15 ]

# Creating the bar plot
plt.figure(figsize=(8, 4))
plt.bar(algorithms, accuracy_scores, color='pink')
plt.title('comparision on PSO')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.ylim(0.2, 1.0)  # Setting y-axis limits
plt.show()

"""Red Panda Optimization"""

import random
import pandas as pd
from sklearn.ensemble import RandomForestClassifier


def red_panda_optimization(data, objective_func, constraints, population_size, iterations):

  features = list(pandas_df.columns)

  # Initialize population
  population = []
  for _ in range(population_size):
    solution = [random.randint(0, 1) for _ in features]  # 0 for not selected, 1 for selected
    population.append(solution)

  # Best solution found so far
  best_solution = []
  best_score = float('-inf')
  selected_food=[]

  for iteration in range(iterations):
    for i in range(population_size):

      # Phase 1: Foraging strategy
      food_positions = [j for j, _ in enumerate(features) if j < population[i][j]]
      if best_solution and any(best_solution):  # Features with lower index (better score)
        valid_indices = [population.index(elem) for elem in best_solution if elem in population]
        food_positions.extend(valid_indices)
      # Choose random element only if food_positions is not empty
      if food_positions:
        selected_food = random.choice(food_positions)
      else:
        selected_food = random.randint(0, len(features) - 1)




      # Update position based on food position
      new_position = []
      for j, _ in enumerate(features):
        step_size = random.random()
        if selected_food is not None and 0 <= selected_food < len(population):
          delta = population[selected_food][j] - population[i][j]
          new_position.append(population[i][j] + step_size * delta)
      else:

          pass

       # Phase 2: Climbing and resting


      # Clip new position to bounds (0 and 1)
      new_position = [max(0, min(1, p)) for p in new_position]

      # Convert solution to feature subset for evaluation
      selected_features = [features[i] for i, f in enumerate(new_position) if f > 0.5]

      # Evaluate fitness based on objective function and constraints
      if not constraints(selected_features):
        continue
      if selected_features:
        fitness = objective_func(selected_features, data)
      else:
        fitness = float('-inf')

      # Update solution if better
      if fitness > best_score:
        best_solution = new_position.copy()
        best_score = fitness



  # Return selected features from best solution
  return [features[i] for i, f in enumerate(best_solution) if f > 0.5]


# Objective function using Random Forest
def objective_func(selected_features, data):
  X = pandas_df[selected_features]  # Features to use for training
  y = pandas_df['Class']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state=30)
  lgt_R = LogisticRegression(random_state = 101, solver='lbfgs', multi_class='auto')
  lgt_R.fit(X_train, y_train)
  accuracy=lgt_R.score(X_test,y_test)
  print('Accuracy:',accuracy)
  return accuracy


def constraints(features):
  return True
# Run RPO
selected_features = red_panda_optimization(data, objective_func, constraints, 10, 100)

print("Selected features:", selected_features)

print(pandas_df.columns)

X_rpo=pandas_df.drop(['Image','Mean','Class','Entropy','Contrast','Energy','ASM','Dissimilarity','Correlation'],axis=1)
y_rpo=pandas_df['Class']

from sklearn.model_selection import train_test_split

X_rpo_train, X_rpo_test, y_rpo_train, y_rpo_test = train_test_split(X_rpo, y_rpo, test_size=0.25, random_state=0)

print("Total data        :",pandas_df.shape[0])
print("No of test data   :",X_rpo_test.shape[0])
print("No of train data  :",X_rpo_train.shape[0])

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()                       #feature scaling
scaled_train = scaler.fit_transform(X_rpo_train)
scaled_test = scaler.fit_transform(X_rpo_test)

pca = PCA(n_components=6)
X_train_pca2 = pca.fit_transform(X_rpo_train)   # Applying PCA to reduce dimensionality
X_test_pca2 = pca.transform(X_rpo_test)

from sklearn.metrics import precision_score
beg_svm = time.time()
svm_classifier = SVC(kernel='rbf', random_state=42)

                                                       # Train SVM classifier
svm_classifier.fit(X_train_pca2, y_rpo_train)

                                         # Prediction
y_pred = svm_classifier.predict(X_test_pca2)
print("            Performance metrics       ")
accuracy21 = accuracy_score(y_rpo_test, y_pred)
print("Accuracy:", accuracy21*100,"%")

end_svm = time.time()

time_svm = end_svm - beg_svm
print("Classification Report:")
print(classification_report(y_rpo_test, y_pred))

print(" Prediction Time ",round(time_svm*1000))

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_rpo_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('SVM Curve')
plt.legend(loc='lower right')
plt.show()

beg_lr=time.time()
model = LogisticRegression()
model.fit(X_train_pca2, y_rpo_train)

                           # Prediction on the test set
y_pred = model.predict(X_test_pca2)

                             # Evaluating the model
accuracy22 = accuracy_score(y_rpo_test, y_pred)
print(f"Accuracy: {accuracy22*100:.2f}")
end_lr=time.time()
time_lr = end_lr - beg_lr


print("Classification Report:")
print(classification_report(y_rpo_test, y_pred))

print("Prediction Time",round(time_lr*1000))

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_rpo_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='purple', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression Curve')
plt.legend(loc='lower right')
plt.show()

beg_nb=time.time()
nb_classifier = GaussianNB()

               # Training Naive Bayes classifier
nb_classifier.fit(X_train_pca2, y_rpo_train)

                   # Predictions
y_pred = nb_classifier.predict(X_test_pca2)

                      # Evaluating the model
accuracy23 = accuracy_score(y_rpo_test, y_pred)
print("Accuracy:", accuracy23*100)
end_nb=time.time()
time_nb = end_nb - beg_nb

print("Classification Report:")
print(classification_report(y_rpo_test, y_pred))

print("Prediction Time",round(time_nb*1000))
# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_rpo_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='green', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Naive bayes Curve')
plt.legend(loc='lower right')
plt.show()

import time

beg_rf=time.time()
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Training Random Forest classifier
rf_classifier.fit(X_train_pca2, y_red_train)

# Prediction
y_pred = rf_classifier.predict(X_test_pca2)

# Evaluating the model
accuracy24 = accuracy_score(y_rpo_test, y_pred)
print("Accuracy:", accuracy24*100)
end_rf=time.time()
time_rf=end_rf - beg_rf

print("Classification Report:")
print(classification_report(y_rpo_test, y_pred))

print("Prediction time",round(time_rf*1000))
# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_rpo_test, y_pred)

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='skyblue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest Curve')
plt.legend(loc='lower right')
plt.show()

beg_st = time.time()
base_models = [
    RandomForestClassifier(),
    GradientBoostingClassifier()
]

base_model_predictions = np.zeros((len(X_test_pca2), len(base_models)))

for i, model in enumerate(base_models):
    model.fit(X_train_pca2, y_rpo_train)
    base_model_predictions[:, i] = model.predict(X_test_pca2)

# Step 2: Train Meta-Model
meta_model = LogisticRegression()
meta_model.fit(base_model_predictions, y_rpo_test)

# Step 3: Make Predictions on Test Set
test_base_model_predictions = np.zeros((len(X_test_pca2), len(base_models)))

for i, model in enumerate(base_models):
    test_base_model_predictions[:, i] = model.predict(X_test_pca2)

final_predictions = meta_model.predict(test_base_model_predictions)

# Step 4: Evaluate
accuracy25 = accuracy_score(y_rpo_test, final_predictions)
print("Accuracy:", accuracy25*100)

end_st=time.time()
time_st=end_st - beg_st

print("prediction Time",round(time_st*1000))
fpr, tpr, thresholds = roc_curve(y_rpo_test, y_pred)

print("Classification Report:")
print(classification_report(y_rpo_test, final_predictions))

# Calculate AUC score
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='gold', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Stacking Curve')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

# List of algorithms
algorithms = ['SVM', 'Logistic Regression', 'Naive bayes', 'Random Forest', 'Stacking']

# Accuracy scores for each algorithm
accuracy_scores = [accuracy21, accuracy22 , accuracy23, accuracy24 , accuracy25 ]

# Creating the bar plot
plt.figure(figsize=(8, 4))
plt.bar(algorithms, accuracy_scores, color='pink')
plt.title('comparision on RPO')
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.ylim(0.2, 1.0)  # Setting y-axis limits
plt.show()